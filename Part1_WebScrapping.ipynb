{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd501d2",
   "metadata": {},
   "source": [
    "## Part 1: Web Scrapping\n",
    "### 2 subreddit pages scrapped using subreddit API (pushshift.io)\n",
    "a. r/marvelstudios\n",
    "b. r/DC_Cinematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8288131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import library\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c187c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrap_subreddit(*args, loop=2, loop_size=100):\n",
    "    \n",
    "  # First part of url\n",
    "  url = 'https://api.pushshift.io//reddit/search/submission' \n",
    "  \n",
    "  # DataFrame initialisation\n",
    "  return_data = pd.DataFrame()\n",
    "  start = time.time()\n",
    "\n",
    "  for subreddit in args:\n",
    "    print(f'Now start scrapping for subreddit: {subreddit}')\n",
    "    combined = []\n",
    "    \n",
    "    for i in range(loop):\n",
    "\n",
    "        if i == 0:\n",
    "          last_post = \"\"\n",
    "        else:\n",
    "          # Get last post time for every loop end as the starting time for new run  \n",
    "          last_post = last_post_time\n",
    "        \n",
    "        # subreddit API parameter grid\n",
    "        params = {\n",
    "         'subreddit': subreddit,\n",
    "         'size':loop_size,\n",
    "         'before':last_post\n",
    "        }\n",
    "\n",
    "        res = requests.get(url,params)\n",
    "        status_code = res.status_code\n",
    "\n",
    "        if status_code != 200:\n",
    "                print(f'Error Occurred, Request Status: {status_code}')\n",
    "                break\n",
    "        else:  \n",
    "               # scrapped data processing - to json file\n",
    "                data = res.json()\n",
    "                post = data['data']\n",
    "                last_post_time = post[-1]['created_utc']\n",
    "                current_time1 = datetime.fromtimestamp(last_post_time)\n",
    "\n",
    "                print(f'{i+1}00 rows of posts are scrapped from {subreddit} up to {current_time1}!')\n",
    "                combined.extend(post) \n",
    "                \n",
    "    # Data are collected in a dataframe\n",
    "    new_data = pd.DataFrame(combined)\n",
    "    return_data = pd.concat([return_data,new_data],axis=0,ignore_index=True)\n",
    "  \n",
    "  print(f'Total RunTime: {time.time() - start}')\n",
    "  return return_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0079d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now start scrapping for subreddit: marvelstudios\n",
      "100 rows of posts are scrapped from marvelstudios up to 2022-07-21 03:39:49!\n",
      "200 rows of posts are scrapped from marvelstudios up to 2022-07-20 21:38:29!\n",
      "300 rows of posts are scrapped from marvelstudios up to 2022-07-20 10:42:33!\n",
      "400 rows of posts are scrapped from marvelstudios up to 2022-07-20 03:18:09!\n",
      "500 rows of posts are scrapped from marvelstudios up to 2022-07-19 19:08:33!\n",
      "600 rows of posts are scrapped from marvelstudios up to 2022-07-19 08:56:35!\n",
      "700 rows of posts are scrapped from marvelstudios up to 2022-07-19 02:10:12!\n",
      "800 rows of posts are scrapped from marvelstudios up to 2022-07-18 18:59:03!\n",
      "900 rows of posts are scrapped from marvelstudios up to 2022-07-18 09:14:18!\n",
      "1000 rows of posts are scrapped from marvelstudios up to 2022-07-18 03:00:53!\n",
      "1100 rows of posts are scrapped from marvelstudios up to 2022-07-17 21:24:41!\n",
      "1200 rows of posts are scrapped from marvelstudios up to 2022-07-17 09:49:21!\n",
      "1300 rows of posts are scrapped from marvelstudios up to 2022-07-17 03:13:23!\n",
      "1400 rows of posts are scrapped from marvelstudios up to 2022-07-16 22:29:08!\n",
      "1500 rows of posts are scrapped from marvelstudios up to 2022-07-16 12:19:45!\n",
      "1600 rows of posts are scrapped from marvelstudios up to 2022-07-16 04:51:22!\n",
      "1700 rows of posts are scrapped from marvelstudios up to 2022-07-15 22:34:13!\n",
      "1800 rows of posts are scrapped from marvelstudios up to 2022-07-15 12:54:05!\n",
      "1900 rows of posts are scrapped from marvelstudios up to 2022-07-15 05:15:47!\n",
      "2000 rows of posts are scrapped from marvelstudios up to 2022-07-14 23:38:59!\n",
      "Now start scrapping for subreddit: DC_Cinematic\n",
      "100 rows of posts are scrapped from DC_Cinematic up to 2022-07-20 07:06:33!\n",
      "200 rows of posts are scrapped from DC_Cinematic up to 2022-07-19 01:25:36!\n",
      "300 rows of posts are scrapped from DC_Cinematic up to 2022-07-17 06:21:34!\n",
      "400 rows of posts are scrapped from DC_Cinematic up to 2022-07-14 12:55:04!\n",
      "500 rows of posts are scrapped from DC_Cinematic up to 2022-07-12 06:49:22!\n",
      "600 rows of posts are scrapped from DC_Cinematic up to 2022-07-10 12:21:18!\n",
      "700 rows of posts are scrapped from DC_Cinematic up to 2022-07-08 09:40:43!\n",
      "800 rows of posts are scrapped from DC_Cinematic up to 2022-07-05 23:49:43!\n",
      "900 rows of posts are scrapped from DC_Cinematic up to 2022-07-03 14:50:26!\n",
      "1000 rows of posts are scrapped from DC_Cinematic up to 2022-07-01 07:39:59!\n",
      "1100 rows of posts are scrapped from DC_Cinematic up to 2022-06-29 01:53:43!\n",
      "1200 rows of posts are scrapped from DC_Cinematic up to 2022-06-27 06:43:40!\n",
      "1300 rows of posts are scrapped from DC_Cinematic up to 2022-06-25 20:22:14!\n",
      "1400 rows of posts are scrapped from DC_Cinematic up to 2022-06-24 03:26:57!\n",
      "1500 rows of posts are scrapped from DC_Cinematic up to 2022-06-22 06:58:14!\n",
      "1600 rows of posts are scrapped from DC_Cinematic up to 2022-06-20 06:42:45!\n",
      "1700 rows of posts are scrapped from DC_Cinematic up to 2022-06-18 21:49:54!\n",
      "1800 rows of posts are scrapped from DC_Cinematic up to 2022-06-17 06:34:34!\n",
      "1900 rows of posts are scrapped from DC_Cinematic up to 2022-06-15 21:30:51!\n",
      "2000 rows of posts are scrapped from DC_Cinematic up to 2022-06-14 16:15:05!\n",
      "Total RunTime: 104.19211769104004\n"
     ]
    }
   ],
   "source": [
    "df = scrap_subreddit('marvelstudios','DC_Cinematic',loop=20,loop_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9b9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./data/raw_scrap.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "49e11d300c81e7b6f6d8423a858ce6950d0920d47d699a698a7b373e1d968c6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
