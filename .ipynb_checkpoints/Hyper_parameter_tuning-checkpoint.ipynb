{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map({'marvelstudios':0, 'DC_Cinematic':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['title_combined']]\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.500251\n",
       "1    0.499749\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default NLTK stopword list\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "# add additional stopwords\n",
    "additional_stopwords = {'like','think','just','new'}\n",
    "stop_words = stop_words.union(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(text):\n",
    "    \n",
    "   # Split and lemmatize words\n",
    "    words = text.split(\" \")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "    # Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    words=[]\n",
    "    for i in (words_lem):   \n",
    "        if i != \"\":\n",
    "            words.append(i)\n",
    "    \n",
    "    no_stop_words = [token for token in words if token not in stop_words]\n",
    "\n",
    "    return (' '.join(no_stop_words))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkfei\\AppData\\Local\\Temp/ipykernel_28820/3434472170.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['title_combined']= X_train['title_combined'].apply(lambda x: lemmatise(x))\n"
     ]
    }
   ],
   "source": [
    "X_train['title_combined']= X_train['title_combined'].apply(lambda x: lemmatise(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow down search range using RandomSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Countvector or Tfidf is better fit as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid for RandomisedCV \n",
    "params_ran_rf = {\n",
    "    'n_estimators': np.arange(10,200,10),\n",
    "    'max_depth' : np.arange(1,6,1),\n",
    "    'min_samples_split': np.arange(2,20,2),\n",
    "    'min_samples_leaf': np.arange(2,10,2),\n",
    "    'max_features': ['None',2,4,6,8,10]\n",
    "}\n",
    "\n",
    "params_ran_nb = {\n",
    "    'alpha':[1,0.9,0.8,0.7,0.6]\n",
    "    \n",
    "}\n",
    "\n",
    "params_ran_dt = {\n",
    "    'max_depth' : np.arange(1,6,1),\n",
    "    'min_samples_split': np.arange(2,20,2),\n",
    "    'min_samples_leaf': np.arange(2,10,2),\n",
    "    'max_features': ['None',2,4,6,8,10]\n",
    "}\n",
    "\n",
    "params_ran_sv = {\n",
    "    'C': np.logspace(-2, 10, 20),\n",
    "    'gamma':['scale',1,0.1,0.01,0.001,0.0001],\n",
    "    'kernel':['rbf','linear','poly']}\n",
    "\n",
    "params_ran_log = {\n",
    "    'penalty': ['l1','l2','none'],\n",
    "    'C': np.logspace(-2, 10, 20),\n",
    "    'class_weight': ['None','balanced']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_result = {}\n",
    "\n",
    "def random_gridsearch(model,parameter,name): \n",
    "\n",
    "  t0 = time.time()\n",
    "  preprocess_step = None\n",
    "  preprocessor_cvec = ColumnTransformer([(\"cvec\", CountVectorizer(),'title_combined')])\n",
    "  preprocessor_tfi = ColumnTransformer([(\"tfidf\", TfidfVectorizer(),'title_combined')])\n",
    "  pipe_ran = Pipeline([('preprocess',preprocessor_cvec),(\"classifier\", model)])\n",
    "\n",
    "  param1 = {}\n",
    "  param1['preprocess'] = [preprocessor_cvec]\n",
    "  param1['preprocess__cvec__max_features'] = np.arange(1500,5000,500)\n",
    "  param1['preprocess__cvec__min_df'] = [1,2,3]\n",
    "  param1['preprocess__cvec__max_df'] = [0.85,0.95,1]\n",
    "  param1['preprocess__cvec__ngram_range'] = [(1,1), (1,2), (1,3)]\n",
    "  for key,value in parameter.items():\n",
    "    param1['classifier'+\"__\"+key] = value\n",
    "\n",
    "\n",
    "  param2 = {}\n",
    "  param2['preprocess'] = [preprocessor_tfi]\n",
    "  param2['preprocess__tfidf__max_features'] = np.arange(1500,5000,500)\n",
    "  param2['preprocess__tfidf__min_df'] = [1,2,3]\n",
    "  param2['preprocess__tfidf__max_df'] = [0.85,0.95,1]\n",
    "  param2['preprocess__tfidf__ngram_range'] = [(1,1), (1,2), (1,3)]\n",
    "  for key,value in parameter.items():\n",
    "    param2['classifier'+\"__\"+key] = value\n",
    "  \n",
    "  params = []\n",
    "  params = [param1, param2] \n",
    "  # run RandomsearchCV\n",
    "  gs_rand = RandomizedSearchCV(pipe_ran, params, cv=3, scoring='accuracy',n_iter=100, n_jobs=-1,verbose=1,random_state=42)\n",
    "  gs_rand.fit(X_train, y_train)\n",
    "  \n",
    "\n",
    "  if 'tfidf' in str(gs_rand.best_params_['preprocess']):\n",
    "    preprocess_step = 'tfvec'\n",
    "  elif 'cvec' in str(gs_rand.best_params_['preprocess']):\n",
    "    preprocess_step = 'cvec'\n",
    "\n",
    "  model = gs_rand.best_estimator_\n",
    "  pred = model.predict(X_test)\n",
    "\n",
    "  tn, fp, fn, tp = confusion_matrix(y_test,\n",
    "                                  pred).ravel()\n",
    "  \n",
    "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "  misclassification = 1 - accuracy\n",
    "  sensitivity = tp / (tp + fn)\n",
    "  specificity = tn / (tn + fp)\n",
    "  precision = tp / (tp + fp)\n",
    "\n",
    "  runtime = time.time() - t0\n",
    "\n",
    "  ran_result[name] = [preprocess_step, model.score(X_train, y_train), model.score(X_test, y_test),accuracy,\n",
    "                           misclassification, sensitivity, specificity, precision, runtime]\n",
    "\n",
    "  print(model)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkfei\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.60983566 0.78130579 0.65741626        nan 0.57903709 0.58707628\n",
      " 0.71634343 0.75418458 0.73645301        nan 0.68887745        nan\n",
      " 0.80274566        nan 0.55190982 0.51273032 0.81413392 0.6922346\n",
      " 0.5        0.74414374        nan 0.64802022 0.56463744 0.64903231\n",
      " 0.50268007 0.56495799 0.64836398 0.52109073        nan        nan\n",
      "        nan 0.755531          nan 0.75719965 0.57031476 0.72973737\n",
      " 0.55559726        nan 0.63564106 0.60449604 0.7967303  0.76054604\n",
      " 0.77762171 0.67045973        nan        nan 0.68386713 0.80675768\n",
      "        nan        nan        nan        nan 0.67881644        nan\n",
      "        nan 0.58941596        nan 0.63094186        nan 0.54722744\n",
      " 0.55727466        nan        nan 0.64568559 0.73309923 0.50033501\n",
      " 0.74683357 0.80978184        nan 0.74279867 0.7642244         nan\n",
      "        nan 0.77261239 0.81949305 0.69054577        nan        nan\n",
      " 0.59811911 0.5        0.69055048        nan 0.5               nan\n",
      "        nan 0.53081203        nan        nan 0.57870813        nan\n",
      " 0.6175311  0.50033501 0.5               nan        nan        nan\n",
      " 0.77696111 0.61890544 0.5        0.81178382]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocess',\n",
      "                 ColumnTransformer(transformers=[('cvec',\n",
      "                                                  CountVectorizer(max_df=0.95,\n",
      "                                                                  max_features=2000,\n",
      "                                                                  min_df=3,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'title_combined')])),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(max_depth=4, max_features=6,\n",
      "                                        min_samples_leaf=2,\n",
      "                                        min_samples_split=18,\n",
      "                                        n_estimators=120))])\n"
     ]
    }
   ],
   "source": [
    "random_gridsearch(RandomForestClassifier(),params_ran_rf,'random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkfei\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.85833518 0.87206615 0.85632547 0.87039078        nan 0.87039078\n",
      " 0.86737637 0.87039078 0.86637236 0.86469832 0.86268726 0.54621669\n",
      " 0.55458921 0.8730695  0.8569965         nan 0.87039044        nan\n",
      "        nan 0.85900486 0.85833586 0.86737671 0.54085723        nan\n",
      "        nan 0.85833586 0.87005644 0.86704103 0.86704237 0.86402831\n",
      " 0.86737604 0.85900621 0.87206515        nan 0.85933954        nan\n",
      " 0.86637135        nan        nan 0.85733083 0.85967623 0.86034523\n",
      " 0.85867019 0.86134757 0.558612   0.86302362 0.85934054        nan\n",
      " 0.8697211  0.55727365 0.87106012 0.86603701 0.87206515 0.87072545\n",
      " 0.56363343 0.86570234 0.85900486        nan 0.85833518 0.85699515\n",
      "        nan 0.85833586 0.86704271 0.85632547        nan 0.8687154\n",
      "        nan 0.86402763 0.8667077         nan 0.86101491 0.86469832\n",
      " 0.86871607        nan 0.86938609 0.87005644 0.85565579 0.558612\n",
      " 0.52679293 0.85833586 0.85766517 0.558612          nan        nan\n",
      " 0.86704237 0.87373951 0.87340518        nan 0.52779762 0.87039044\n",
      " 0.87206515        nan 0.8667077         nan 0.8667077         nan\n",
      " 0.558612   0.87005644 0.86938676 0.86938676]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocess',\n",
      "                 ColumnTransformer(transformers=[('tfidf',\n",
      "                                                  TfidfVectorizer(max_df=0.95,\n",
      "                                                                  max_features=4500,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               2)),\n",
      "                                                  'title_combined')])),\n",
      "                ('classifier', MultinomialNB(alpha=0.8))])\n"
     ]
    }
   ],
   "source": [
    "random_gridsearch(MultinomialNB(),params_ran_nb,'multinomial_Naive_Bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkfei\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.50502378 0.50234506 0.50066934 0.54254102 0.50200971 0.5170824\n",
      " 0.50468844        nan 0.51439123 0.54655372        nan 0.50636314\n",
      " 0.51037853 0.50033501 0.52445225        nan        nan        nan\n",
      " 0.50033501 0.50066968 0.51272796 0.50937923        nan 0.50368408\n",
      " 0.527802   0.51574304        nan 0.51271586 0.50100503 0.50033501\n",
      " 0.5103819  0.50368375 0.50066968 0.52411758        nan 0.50569043\n",
      " 0.51440166 0.50133936        nan        nan        nan        nan\n",
      "        nan 0.51875879        nan        nan        nan        nan\n",
      " 0.50033501 0.50435275        nan        nan        nan        nan\n",
      "        nan        nan 0.50903816 0.50401909        nan        nan\n",
      " 0.51105528        nan 0.51205391 0.50134003        nan 0.5043541\n",
      "        nan        nan        nan 0.50033501 0.50971524        nan\n",
      "        nan 0.50937553        nan 0.50033501 0.51507235 0.50033501\n",
      "        nan        nan        nan 0.50970986 0.5110435  0.50267771\n",
      " 0.53180898 0.50669882 0.51741808 0.50033501 0.5        0.50100503\n",
      "        nan        nan 0.50067002 0.50535946 0.50669882        nan\n",
      " 0.50268007 0.50033501        nan 0.50334605]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocess',\n",
      "                 ColumnTransformer(transformers=[('cvec',\n",
      "                                                  CountVectorizer(max_df=0.95,\n",
      "                                                                  max_features=4500,\n",
      "                                                                  min_df=3),\n",
      "                                                  'title_combined')])),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(max_depth=4, max_features=4,\n",
      "                                        min_samples_leaf=2,\n",
      "                                        min_samples_split=14))])\n"
     ]
    }
   ],
   "source": [
    "random_gridsearch(DecisionTreeClassifier(),params_ran_dt,'decision_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Pipeline(steps=[('preprocess',\n",
      "                 ColumnTransformer(transformers=[('tfidf',\n",
      "                                                  TfidfVectorizer(max_df=0.95,\n",
      "                                                                  max_features=2000,\n",
      "                                                                  min_df=2),\n",
      "                                                  'title_combined')])),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=3.359818286283781, class_weight='None'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkfei\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.84058277 0.83689633        nan 0.53014302        nan        nan\n",
      " 0.8439352         nan 0.84159116 0.85130203        nan 0.84661494\n",
      " 0.57300593 0.83957505        nan 0.86469967 0.84895865        nan\n",
      " 0.55459257 0.84326452 0.58003841        nan 0.8744102         nan\n",
      "        nan 0.83020659 0.83690608 0.84159116 0.8352223  0.8439352\n",
      "        nan 0.84962597        nan        nan        nan        nan\n",
      " 0.84293119 0.83824578        nan        nan 0.83020491 0.83456271\n",
      " 0.83556571 0.83020558 0.8523084  0.84326552 0.83824544 0.84226151\n",
      "        nan 0.55559222 0.8620196         nan        nan        nan\n",
      "        nan 0.84895966 0.83724277 0.84159048 0.58405717        nan\n",
      "        nan 0.84493855 0.83523138 0.84962833        nan 0.84962799\n",
      "        nan        nan        nan        nan 0.83690137 0.833222\n",
      " 0.84694692 0.83154662 0.8365704  0.86637303        nan        nan\n",
      " 0.83724076 0.833222   0.83321628        nan 0.84694994        nan\n",
      " 0.82752249 0.82853592 0.84393352 0.83590207 0.83421996        nan\n",
      "        nan 0.83523272        nan        nan 0.86335728 0.53583614\n",
      " 0.84225949        nan 0.82886487 0.83656569]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "random_gridsearch(LogisticRegression(),params_ran_log,'logistic_regre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "random_gridsearch(SVC(),params_ran_sv,'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomial_Naive_Bayes': ['tfvec',\n",
       "  0.955793703951775,\n",
       "  0.8534136546184738,\n",
       "  0.8534136546184738,\n",
       "  0.14658634538152615,\n",
       "  0.8172690763052208,\n",
       "  0.8895582329317269,\n",
       "  0.8809523809523809,\n",
       "  7.637492895126343],\n",
       " 'decision_tree': ['cvec',\n",
       "  0.5060281312793035,\n",
       "  0.5090361445783133,\n",
       "  0.5090361445783133,\n",
       "  0.49096385542168675,\n",
       "  1.0,\n",
       "  0.018072289156626505,\n",
       "  0.5045592705167173,\n",
       "  6.98026442527771],\n",
       " 'random_forest': ['cvec',\n",
       "  0.825853985264568,\n",
       "  0.7901606425702812,\n",
       "  0.7901606425702812,\n",
       "  0.20983935742971882,\n",
       "  0.642570281124498,\n",
       "  0.9377510040160643,\n",
       "  0.9116809116809117,\n",
       "  16.61593270301819],\n",
       " 'logistic_regre': ['tfvec',\n",
       "  0.9671801741460148,\n",
       "  0.8524096385542169,\n",
       "  0.8524096385542169,\n",
       "  0.14759036144578308,\n",
       "  0.8514056224899599,\n",
       "  0.8534136546184738,\n",
       "  0.8531187122736419,\n",
       "  11.620300531387329]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocess__tfidf__ngram_range': (1, 1),\n",
       " 'preprocess__tfidf__min_df': 1,\n",
       " 'preprocess__tfidf__max_features': 2500,\n",
       " 'preprocess__tfidf__max_df': 0.85,\n",
       " 'preprocess': ColumnTransformer(transformers=[('tfidf',\n",
       "                                  TfidfVectorizer(max_df=0.85,\n",
       "                                                  max_features=2500),\n",
       "                                  'title_combined')]),\n",
       " 'classifier__alpha': 0.8}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rand.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs_rand.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('preprocess',\n",
       "                 ColumnTransformer(transformers=[('tfidf',\n",
       "                                                  TfidfVectorizer(max_df=0.85,\n",
       "                                                                  max_features=2500),\n",
       "                                                  'title_combined')])),\n",
       "                ('classifier', MultinomialNB(alpha=0.8))])>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocess__tfidf__ngram_range</th>\n",
       "      <th>param_preprocess__tfidf__min_df</th>\n",
       "      <th>param_preprocess__tfidf__max_features</th>\n",
       "      <th>param_preprocess__tfidf__max_df</th>\n",
       "      <th>param_preprocess</th>\n",
       "      <th>param_classifier__alpha</th>\n",
       "      <th>...</th>\n",
       "      <th>param_preprocess__cvec__min_df</th>\n",
       "      <th>param_preprocess__cvec__max_features</th>\n",
       "      <th>param_preprocess__cvec__max_df</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.048676</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>0.021675</td>\n",
       "      <td>0.004110</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>1</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.85</td>\n",
       "      <td>ColumnTransformer(transformers=[('tfidf',\\n   ...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'preprocess__tfidf__ngram_range': (1, 1), 'pr...</td>\n",
       "      <td>0.886546</td>\n",
       "      <td>0.865327</td>\n",
       "      <td>0.866332</td>\n",
       "      <td>0.872735</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.137221</td>\n",
       "      <td>0.019368</td>\n",
       "      <td>0.026673</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>ColumnTransformer(transformers=[('tfidf',\\n   ...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'preprocess__tfidf__ngram_range': (1, 2), 'pr...</td>\n",
       "      <td>0.883534</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.872401</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.068348</td>\n",
       "      <td>0.016135</td>\n",
       "      <td>0.038804</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>ColumnTransformer(transformers=[('tfidf',\\n   ...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'preprocess__tfidf__ngram_range': (1, 1), 'pr...</td>\n",
       "      <td>0.884538</td>\n",
       "      <td>0.866332</td>\n",
       "      <td>0.863317</td>\n",
       "      <td>0.871395</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.122971</td>\n",
       "      <td>0.012255</td>\n",
       "      <td>0.028340</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>ColumnTransformer(transformers=[('tfidf',\\n   ...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'preprocess__tfidf__ngram_range': (1, 2), 'pr...</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.863317</td>\n",
       "      <td>0.871060</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.129364</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.022336</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ColumnTransformer(transformers=[('cvec', Count...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>{'preprocess__cvec__ngram_range': (1, 2), 'pre...</td>\n",
       "      <td>0.881526</td>\n",
       "      <td>0.873367</td>\n",
       "      <td>0.855276</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "9        0.048676      0.005313         0.021675        0.004110   \n",
       "37       0.137221      0.019368         0.026673        0.000940   \n",
       "13       0.068348      0.016135         0.038804        0.008193   \n",
       "0        0.122971      0.012255         0.028340        0.010500   \n",
       "5        0.129364      0.008223         0.022336        0.000470   \n",
       "\n",
       "   param_preprocess__tfidf__ngram_range param_preprocess__tfidf__min_df  \\\n",
       "9                                (1, 1)                               1   \n",
       "37                               (1, 2)                               1   \n",
       "13                               (1, 1)                               2   \n",
       "0                                (1, 2)                               1   \n",
       "5                                   NaN                             NaN   \n",
       "\n",
       "   param_preprocess__tfidf__max_features param_preprocess__tfidf__max_df  \\\n",
       "9                                   2500                            0.85   \n",
       "37                                  4000                            0.95   \n",
       "13                                  2000                            0.85   \n",
       "0                                   3000                            0.85   \n",
       "5                                    NaN                             NaN   \n",
       "\n",
       "                                     param_preprocess param_classifier__alpha  \\\n",
       "9   ColumnTransformer(transformers=[('tfidf',\\n   ...                     0.8   \n",
       "37  ColumnTransformer(transformers=[('tfidf',\\n   ...                     0.8   \n",
       "13  ColumnTransformer(transformers=[('tfidf',\\n   ...                     0.9   \n",
       "0   ColumnTransformer(transformers=[('tfidf',\\n   ...                     0.9   \n",
       "5   ColumnTransformer(transformers=[('cvec', Count...                       1   \n",
       "\n",
       "    ... param_preprocess__cvec__min_df param_preprocess__cvec__max_features  \\\n",
       "9   ...                            NaN                                  NaN   \n",
       "37  ...                            NaN                                  NaN   \n",
       "13  ...                            NaN                                  NaN   \n",
       "0   ...                            NaN                                  NaN   \n",
       "5   ...                              1                                 4000   \n",
       "\n",
       "   param_preprocess__cvec__max_df  \\\n",
       "9                             NaN   \n",
       "37                            NaN   \n",
       "13                            NaN   \n",
       "0                             NaN   \n",
       "5                            0.95   \n",
       "\n",
       "                                               params split0_test_score  \\\n",
       "9   {'preprocess__tfidf__ngram_range': (1, 1), 'pr...          0.886546   \n",
       "37  {'preprocess__tfidf__ngram_range': (1, 2), 'pr...          0.883534   \n",
       "13  {'preprocess__tfidf__ngram_range': (1, 1), 'pr...          0.884538   \n",
       "0   {'preprocess__tfidf__ngram_range': (1, 2), 'pr...          0.885542   \n",
       "5   {'preprocess__cvec__ngram_range': (1, 2), 'pre...          0.881526   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "9            0.865327           0.866332         0.872735        0.009775   \n",
       "37           0.869347           0.864322         0.872401        0.008135   \n",
       "13           0.866332           0.863317         0.871395        0.009374   \n",
       "0            0.864322           0.863317         0.871060        0.010249   \n",
       "5            0.873367           0.855276         0.870056        0.010969   \n",
       "\n",
       "    rank_test_score  \n",
       "9                 1  \n",
       "37                2  \n",
       "13                3  \n",
       "0                 4  \n",
       "5                 5  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_rand.cv_results_).sort_values('rank_test_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28820/2401537416.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m plot_confusion_matrix(model,\n\u001b[0m\u001b[0;32m      4\u001b[0m                       \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                       \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(model,\n",
    "                      X_test,\n",
    "                      y_test,\n",
    "                      values_format='d',\n",
    "                      display_labels=['Marvel','DC'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "49e11d300c81e7b6f6d8423a858ce6950d0920d47d699a698a7b373e1d968c6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
